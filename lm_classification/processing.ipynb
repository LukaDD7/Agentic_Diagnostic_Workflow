{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import LongformerForSequenceClassification, LongformerConfig, LongformerTokenizer\n",
    "\n",
    "# Step 1: Load Pretrained Model and Tokenizer\n",
    "model_name = \"yikuan8/Clinical-Longformer\" # \"allenai/longformer-base-4096\"\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define config with correct label count\n",
    "config = LongformerConfig.from_pretrained(model_name)\n",
    "config.num_labels = 4\n",
    "\n",
    "# Load base model with classification head\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:80} requires_grad={param.requires_grad}, shape={param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:80} requires_grad={param.requires_grad}, shape={param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if any([\n",
    "        name.startswith(\"longformer.encoder.layer.10\"),\n",
    "        name.startswith(\"longformer.encoder.layer.11\"),\n",
    "        name.startswith(\"classifier\"),\n",
    "        \"LayerNorm\" in name,\n",
    "    ]):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:80} requires_grad={param.requires_grad}, shape={param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import AcuteAbdominalDiagnosisDataset, get_dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"/media/luzhenyang/project/agent_graph_diag/lm_classification/ab_cls_dataset_v2_complete_info.csv\")\n",
    "label_list = df['diagnosis'].unique().tolist()\n",
    "print(\"label_list: \", label_list)\n",
    "\n",
    "# Tokenizer + Padding + Truncation 处理\n",
    "encoded = [\n",
    "    tokenizer(\n",
    "        text, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        # return_tensors='pt' 会导致 input_ids shape: torch.Size([4, 1, 4096])\n",
    "    ) for text in tqdm(df['context'].tolist())\n",
    "]\n",
    "\n",
    "df['input_ids'] = [ e['input_ids'] for e in encoded ]\n",
    "df['attention_mask'] = [ e['attention_mask'] for e in encoded ]\n",
    "df['input_length'] = df['context'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# 划分训练集，测试集\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['diagnosis'], random_state=7)\n",
    "\n",
    "print(train_df['diagnosis'].value_counts())\n",
    "print(val_df['diagnosis'].value_counts())\n",
    "\n",
    "# 列：['context', 'diagnosis']\n",
    "train_dataset = AcuteAbdominalDiagnosisDataset(train_df)\n",
    "val_dataset = AcuteAbdominalDiagnosisDataset(val_df)\n",
    "train_loader = get_dataloader(train_dataset, use_weighted=True)\n",
    "val_loader = get_dataloader(val_dataset, use_weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 在优化器之后，添加 Scheduler 设置 ----\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "MAX_LENGTH = 4096\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# 计算总的训练步数\n",
    "num_training_steps = len(train_loader) * NUM_EPOCHS\n",
    "num_warmup_steps = int(0.1 * num_training_steps)  # 通常设为 10% 的 warmup\n",
    "print(num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Huggingface 提供的线性调度器\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=num_warmup_steps, \n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "torch.cuda.set_device(2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_torch(device=device)\n",
    "model.to(device)\n",
    "\n",
    "# ---------- 更新训练函数：加入 scheduler.step() ----------\n",
    "def train_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        global_attention_mask = torch.zeros_like(input_ids)\n",
    "        global_attention_mask[:, 0] = 1\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            global_attention_mask=global_attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 更新学习率\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "log_dir = \"/media/luzhenyang/project/agent_graph_diag/lm_classification/training_logs_longformer\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_txt_path = os.path.join(log_dir, f\"log_{timestamp}.txt\")\n",
    "log_csv_path = os.path.join(log_dir, f\"log_{timestamp}.csv\")\n",
    "\n",
    "csv_headers = [\"epoch\", \"train_loss\", \"micro_f1\", \"macro_f1\", \"accuracy\"]\n",
    "with open(log_csv_path, \"w\") as f:\n",
    "    f.write(\",\".join(csv_headers) + \"\\n\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "def per_class_accuracy(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))\n",
    "    total = cm.sum()\n",
    "    per_class_acc = {}\n",
    "    for i, cls in enumerate(class_names):\n",
    "        TP = cm[i, i]\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        FN = cm[i, :].sum() - TP\n",
    "        TN = total - TP - FP - FN\n",
    "        accuracy_i = (TP + TN) / total\n",
    "        per_class_acc[cls] = accuracy_i\n",
    "    return per_class_acc\n",
    "\n",
    "def log_metrics(epoch, train_loss, y_true, y_pred, log_txt_path, log_csv_path):\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 计算每个类别准确率\n",
    "    per_class_acc = per_class_accuracy(y_true, y_pred, label_list)\n",
    "\n",
    "    # 文本日志\n",
    "    with open(log_txt_path, \"a\") as f:\n",
    "        f.write(f\"\\nEpoch {epoch+1}\\n\")\n",
    "        f.write(f\"Train Loss: {train_loss:.4f}\\n\")\n",
    "        f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "        f.write(f\"Micro F1: {micro_f1:.4f} | Macro F1: {macro_f1:.4f}\\n\")\n",
    "\n",
    "        # 每个类别准确率写入日志\n",
    "        for cls, acc_cls in per_class_acc.items():\n",
    "            f.write(f\"Accuracy for {cls}: {acc_cls:.4f}\\n\")\n",
    "\n",
    "        f.write(f\"{classification_report(y_true, y_pred, target_names=label_list, digits=4)}\\n\")\n",
    "\n",
    "    # CSV日志\n",
    "    with open(log_csv_path, \"a\") as f:\n",
    "        f.write(f\"{epoch+1},{train_loss:.4f},{micro_f1:.4f},{macro_f1:.4f},{acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 验证函数 ----------\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            global_attention_mask[:, 0] = 1\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                global_attention_mask=global_attention_mask\n",
    "            )\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=label_list, digits=4)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 主训练循环 ----------\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # 验证并获取预测与标签\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # 打印+写入日志\n",
    "    log_metrics(epoch, train_loss, all_labels, all_preds, log_txt_path, log_csv_path)\n",
    "\n",
    "    # 保存模型\n",
    "    save_path = f\"checkpoint_epoch{epoch+1}.pt\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对齐AGAP的实验设置，验证集80例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "# ---------- 更新训练函数：加入 scheduler.step() ----------\n",
    "def train_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        global_attention_mask = torch.zeros_like(input_ids)\n",
    "        global_attention_mask[:, 0] = 1\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            global_attention_mask=global_attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 更新学习率\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "log_dir = \"/media/luzhenyang/project/agent_graph_diag/lm_classification/training_logs_longformer_random_seeds\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def per_class_accuracy(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))\n",
    "    total = cm.sum()\n",
    "    per_class_acc = {}\n",
    "    for i, cls in enumerate(class_names):\n",
    "        TP = cm[i, i]\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        FN = cm[i, :].sum() - TP\n",
    "        TN = total - TP - FP - FN\n",
    "        accuracy_i = (TP + TN) / total\n",
    "        per_class_acc[cls] = accuracy_i\n",
    "    return per_class_acc\n",
    "\n",
    "def log_metrics(epoch, train_loss, y_true, y_pred, log_txt_path, log_csv_path):\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 计算每个类别准确率\n",
    "    per_class_acc = per_class_accuracy(y_true, y_pred, label_list)\n",
    "\n",
    "    # 文本日志\n",
    "    with open(log_txt_path, \"a\") as f:\n",
    "        f.write(f\"\\nEpoch {epoch+1}\\n\")\n",
    "        f.write(f\"Train Loss: {train_loss:.4f}\\n\")\n",
    "        f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "        f.write(f\"Micro F1: {micro_f1:.4f} | Macro F1: {macro_f1:.4f}\\n\")\n",
    "\n",
    "        # 每个类别准确率写入日志\n",
    "        for cls, acc_cls in per_class_acc.items():\n",
    "            f.write(f\"Accuracy for {cls}: {acc_cls:.4f}\\n\")\n",
    "\n",
    "        f.write(f\"{classification_report(y_true, y_pred, target_names=label_list, digits=4)}\\n\")\n",
    "\n",
    "    # CSV日志\n",
    "    with open(log_csv_path, \"a\") as f:\n",
    "        f.write(f\"{epoch+1},{train_loss:.4f},{micro_f1:.4f},{macro_f1:.4f},{acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 验证函数 ----------\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            global_attention_mask[:, 0] = 1\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                global_attention_mask=global_attention_mask\n",
    "            )\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=label_list, digits=4)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对齐ADW的实验设置，验证集80例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    LongformerForSequenceClassification,\n",
    "    LongformerConfig,\n",
    "    LongformerTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "\n",
    "def setup_model_and_optimizer(train_loader,\n",
    "                              model_name=\"yikuan8/Clinical-Longformer\",\n",
    "                              num_labels=4,\n",
    "                              lr=2e-5,\n",
    "                              weight_decay=0.01,\n",
    "                              num_epochs=3,\n",
    "                              freeze_except_last=True,\n",
    "                              device=None):\n",
    "    \"\"\"\n",
    "    初始化 Longformer 模型、优化器与学习率调度器。\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1. 加载 tokenizer 和 config\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "    config = LongformerConfig.from_pretrained(model_name)\n",
    "    config.num_labels = num_labels\n",
    "\n",
    "    # 2. 加载模型\n",
    "    model = LongformerForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "    # 3. 冻结部分层（可选）\n",
    "    if freeze_except_last:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if any([\n",
    "                name.startswith(\"longformer.encoder.layer.10\"),\n",
    "                name.startswith(\"longformer.encoder.layer.11\"),\n",
    "                name.startswith(\"classifier\"),\n",
    "                \"LayerNorm\" in name,\n",
    "            ]):\n",
    "                param.requires_grad = True\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # 4. 设置优化器\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # 5. 设置调度器\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # 打印信息\n",
    "    print(f\"[✓] Loaded model: {model_name}\")\n",
    "    print(f\"[✓] Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    print(f\"[✓] Total steps: {num_training_steps}, Warmup: {num_warmup_steps}\")\n",
    "\n",
    "    return model, tokenizer, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils import AcuteAbdominalDiagnosisDataset, get_dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_LENGTH = 4096\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "model_name = \"yikuan8/Clinical-Longformer\" # \"allenai/longformer-base-4096\"\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "\n",
    "df = pd.read_csv(\"/media/luzhenyang/project/agent_graph_diag/lm_classification/ab_cls_dataset_v2_complete_info.csv\")\n",
    "label_list = df['diagnosis'].unique().tolist()\n",
    "print(\"label_list: \", label_list)\n",
    "\n",
    "# Tokenizer + Padding + Truncation 处理\n",
    "encoded = [\n",
    "    tokenizer(\n",
    "        text, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        # return_tensors='pt' 会导致 input_ids shape: torch.Size([4, 1, 4096])\n",
    "    ) for text in tqdm(df['context'].tolist())\n",
    "]\n",
    "\n",
    "df['input_ids'] = [ e['input_ids'] for e in encoded ]\n",
    "df['attention_mask'] = [ e['attention_mask'] for e in encoded ]\n",
    "df['input_length'] = df['context'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "# 根据随机种子id划分测试集、训练集\n",
    "ran_seeds = [1,4,7,9,10,20,23,42,71,96]\n",
    "\n",
    "for seed in ran_seeds:\n",
    "    print(f\"\\n====== Seed: {seed} ======\")\n",
    "    val_ids = pd.read_csv(f\"/media/luzhenyang/project/agent_graph_diag/subset_ids_{seed}.csv\")\n",
    "    val_df = df[ df['hadm_id'].isin(val_ids['hadm_id'].values) ]\n",
    "    train_df = df[ ~df['hadm_id'].isin(val_ids['hadm_id'].values) ]\n",
    "\n",
    "    print(train_df['diagnosis'].value_counts())\n",
    "    print(val_df['diagnosis'].value_counts())\n",
    "\n",
    "    # 列：['context', 'diagnosis']\n",
    "    train_dataset = AcuteAbdominalDiagnosisDataset(train_df)\n",
    "    val_dataset = AcuteAbdominalDiagnosisDataset(val_df)\n",
    "    train_loader = get_dataloader(train_dataset, use_weighted=True)\n",
    "    val_loader = get_dataloader(val_dataset, use_weighted=False)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_txt_path = os.path.join(log_dir, f\"log_{timestamp}_{seed}.txt\")\n",
    "    log_csv_path = os.path.join(log_dir, f\"log_{timestamp}_{seed}.csv\")\n",
    "\n",
    "    csv_headers = [\"epoch\", \"train_loss\", \"micro_f1\", \"macro_f1\", \"accuracy\"]\n",
    "    with open(log_csv_path, \"w\") as f:\n",
    "        f.write(\",\".join(csv_headers) + \"\\n\")\n",
    "\n",
    "    # ---------- 主训练循环 ----------\n",
    "    # os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    model, tokenizer, optimizer, scheduler = setup_model_and_optimizer(train_loader)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}_{seed}\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # 验证并获取预测与标签\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().tolist())\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        # 打印+写入日志\n",
    "        log_metrics(epoch, train_loss, all_labels, all_preds, log_txt_path, log_csv_path)\n",
    "\n",
    "        # 保存模型\n",
    "        save_path = f\"{log_dir}/checkpoint_epoch{epoch+1}_{seed}.pt\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/media/luzhenyang/project/agent_graph_diag/lm_classification/ab_cls_dataset.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data = pd.read_csv('/media/luzhenyang/project/agent_graph_diag/AGAP_full_dataset_results/AGAP_full_dataset_own.csv')\n",
    "inter_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/media/luzhenyang/project/agent_graph_diag/AGAP')\n",
    "\n",
    "from extract_patient_info import Template_customized\n",
    "\n",
    "patient_info_path = '/media/luzhenyang/project/datasets/mimic_iv_ext_clinical_decision_abdominal/clinical_decision_making_for_abdominal_pathologies_1.1'\n",
    "patient_info_file_names = [\n",
    "    'history_of_present_illness.csv', \n",
    "    'microbiology.csv',\n",
    "    'laboratory_tests.csv',\n",
    "    'radiology_reports.csv',\n",
    "]\n",
    "\n",
    "patients_info = Template_customized(\n",
    "    base_path=patient_info_path,\n",
    "    file_names=patient_info_file_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = inter_data[['hadm_id', 'diagnosis']].copy()\n",
    "df_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推荐写法\n",
    "def extract_all(row):\n",
    "    return pd.Series({\n",
    "        'hpi': patients_info.extract_hpi(row['hadm_id']),\n",
    "        'pe': patients_info.extract_pe(row['hadm_id']),\n",
    "        'lab': patients_info.laboratory_test_mapping_v2_llm(row['hadm_id']),\n",
    "        'ima': patients_info.extract_rr(row['hadm_id'])\n",
    "    })\n",
    "\n",
    "df_dataset[['hpi', 'pe', 'lab', 'ima']] = df_dataset.apply(extract_all, axis=1)\n",
    "\n",
    "# for id in tqdm(df_dataset['hadm_id'].values):\n",
    "#     df_dataset['hpi'] = df_dataset.apply(\n",
    "#         lambda id: patients_info.extract_hpi(hadm_id=id) \n",
    "#     )\n",
    "#     df_dataset['pe'] = df_dataset.apply(\n",
    "#         lambda id: patients_info.extract_pe(hadm_id=id)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset['lab'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_concat = ['hpi', 'pe', 'lab', 'ima']\n",
    "\n",
    "df_dataset['context'] = df_dataset[cols_to_concat].astype(str).agg('\\n'.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset['context'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计平均tokens数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import LongformerForSequenceClassification, LongformerConfig, LongformerTokenizer\n",
    "\n",
    "# Step 1: Load Pretrained Model and Tokenizer\n",
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "\n",
    "df_dataset['token_count'] = df_dataset['context'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "avg_len = df_dataset['token_count'].mean()\n",
    "max_len = df_dataset['token_count'].max()\n",
    "percentiles = df_dataset['token_count'].quantile([0.5, 0.9, 0.95, 0.99])\n",
    "\n",
    "print(f\"平均 token 数量: {avg_len:.2f}\")\n",
    "print(f\"最大 token 数量: {max_len}\")\n",
    "print(\"分布分位数:\")\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.to_csv('/media/luzhenyang/project/agent_graph_diag/lm_classification/ab_cls_dataset_v2_complete_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag_diag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
